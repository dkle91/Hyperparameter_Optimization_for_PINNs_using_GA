{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wave equation - Grid Search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNr2tCosqIevNk6FTJ7Qiv4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkle91/Hyperparameter_Optimization_for_PINNs_using_GA/blob/main/Wave_equation_Grid_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQdsnflLKkmb"
      },
      "outputs": [],
      "source": [
        "#Import TensorFlow and NumPy\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from keras import models \n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from google.colab import files\n",
        "\n",
        "# Set data type\n",
        "DTYPE='float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "\n",
        "# Set constants\n",
        "pi = tf.constant(np.pi, dtype=DTYPE)\n",
        "c2 = 1\n",
        "\n",
        "# Define initial & boundary conditions\n",
        "def u_0(x):\n",
        "    return tf.sin(pi*x)\n",
        "def v_t_0(x):\n",
        "    return -pi*tf.sin(pi*x)\n",
        "\n",
        "# Define residual of the PDE\n",
        "def fun_r(t, x, u, u_tt, u_xx):\n",
        "    return u_tt - c2 * u_xx\n",
        "\n",
        "# Define exact solution function\n",
        "def fun_exact(t, x):\n",
        "    return tf.sin(pi*x)*tf.cos(pi*t) - tf.sin(pi*x)*tf.sin(pi*t)\n",
        "\n",
        "\n",
        "# Set boundary\n",
        "tmin = 0.\n",
        "tmax = 1.\n",
        "xmin = 0.\n",
        "xmax = 1.\n",
        "\n",
        "# Lower bounds\n",
        "lb = tf.constant([tmin, xmin], dtype=DTYPE)\n",
        "# Upper bounds\n",
        "ub = tf.constant([tmax, xmax], dtype=DTYPE)\n",
        "\n",
        "# Define t,x for evaluation and exact solutions\n",
        "num_eval = 100\n",
        "t_eval = tf.linspace(lb[0], ub[0], num_eval)\n",
        "x_eval = tf.linspace(lb[1], ub[1], num_eval)\n",
        "T_eval,X_eval = np.meshgrid(t_eval, x_eval) # make X_train [[x1 x2 x3 ... xn], [x1 x2 x3 ...xn],...(Nt number)....[x1 x2 x3 ... xn]] and make T_train [[t1 t1 t1 ... t1], [t2 t2 t2 ...t2],...(Nx number)....[tn tn tn ... tn]]\n",
        "teval = T_eval.ravel() # make it one row\n",
        "teval = tf.reshape(tf.convert_to_tensor(teval),shape=(-1,1)) # Convert it to tensor and make it one column\n",
        "xeval = X_eval.ravel() # make it one row\n",
        "xeval = tf.reshape(tf.convert_to_tensor(xeval),shape=(-1,1)) # Convert it to tensor and make it one column\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Result management\n",
        "Result = np.ones((16384, 11))*999999\n",
        "i= 0\n",
        "\n",
        "# a. Number of train points in domain: 0-50, 1-100, 2-150, 3-200\n",
        "num_train_domain = [50, 100, 150, 200]\n",
        "# b. Activation function: 0-tanh, 1-sigmoid, 2-Relu, 3-selu\n",
        "activation = ['tanh', 'sigmoid', 'relu', 'selu']\n",
        "# c. Optimizer: 0-Adam, 1-SGD, 2-RMSprop, 3-Adadelta\n",
        "optimizer = ['Adam', 'SGD', 'RMSprop', 'Adadelta']\n",
        "# d. Learning rate: 0-1e-2, 1-5e-3, 2-1e-3, 3-5e-4\n",
        "learn_rate = [1e-02, 5e-03, 1e-03, 5e-4]\n",
        "# e. Number of epochs: 0-2500, 1-5000, 2-7500, 3-10000\n",
        "N_epoch = [2500, 5000, 7500, 10000]\n",
        "# f. Number of neurons: 0-50, 1-100, 2-150, 3-20\n",
        "num_neurons_per_layer = [5, 10, 15, 20]\n",
        "# g. Number of layers: 0-1, 1-2, 2-3, 3-4\n",
        "num_hidden_layers = [1, 2, 3, 4]\n",
        "\n",
        "\n",
        "def init_model(num_hidden_layers, num_neurons_per_layer):\n",
        "    # Initialize a feedforward neural network\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Input is two-dimensional (time + one spatial dimension)\n",
        "    model.add(tf.keras.Input(2))\n",
        "\n",
        "    # Introduce a scaling layer to map input to [lb, ub]\n",
        "    scaling_layer = tf.keras.layers.Lambda(\n",
        "                lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
        "    model.add(scaling_layer)\n",
        "\n",
        "    # Append hidden layers\n",
        "    for _ in range(num_hidden_layers):\n",
        "        model.add(tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "            activation=tf.keras.activations.get(activation_func),\n",
        "            kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Output is one-dimensional\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    \n",
        "    return model\n",
        "\n",
        "def get_r(model, X_r):\n",
        "    # A tf.GradientTape is used to compute derivatives in TensorFlow\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Split t and x to compute partial derivatives\n",
        "        t, x = X_r[:, 0:1], X_r[:,1:2]\n",
        "\n",
        "        # Variables t and x are watched during tape\n",
        "        # to compute derivatives u_t and u_x\n",
        "        tape.watch(t)\n",
        "        tape.watch(x)\n",
        "\n",
        "        # Determine residual \n",
        "        u = (1 - t**2)*u_0(x) + t*v_t_0(x) + x*(1-x)*t**2*model(tf.stack([t[:,0], x[:,0]], axis=1))\n",
        "\n",
        "        # Compute gradient u_x within the GradientTape\n",
        "        # since we need second derivatives\n",
        "        u_x = tape.gradient(u, x)        \n",
        "        u_t = tape.gradient(u, t)\n",
        "    u_xx = tape.gradient(u_x, x)\n",
        "    u_tt = tape.gradient(u_t, t)\n",
        "    del tape\n",
        "    return fun_r(t, x, u, u_tt, u_xx)\n",
        "\n",
        "def compute_loss(model, X_r):\n",
        "    # Compute phi^r\n",
        "    r = get_r(model, X_r)\n",
        "    phi_r = tf.reduce_mean(tf.square(r))\n",
        "    # Initialize loss\n",
        "    loss = phi_r\n",
        "    return loss\n",
        "\n",
        "def get_grad(model, X_r):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # This tape is for derivatives with\n",
        "        # respect to trainable variables\n",
        "        tape.watch(model.trainable_variables)\n",
        "        loss = compute_loss(model, X_r)\n",
        "    g = tape.gradient(loss, model.trainable_variables)\n",
        "    del tape\n",
        "    return loss, g\n",
        "\n",
        "# Calculation for exact solutions\n",
        "exact = fun_exact(teval, xeval)\n",
        "Exact = np.reshape(exact,(num_eval, num_eval))\n",
        "Teval, Xeval = np.meshgrid(t_eval, x_eval)\n",
        "Eval_grid = np.vstack([Teval.flatten(),Xeval.flatten()]).T\n",
        "\n",
        "# Surface plot of exact solution u(t,x)\n",
        "#fig = plt.figure(figsize=(9,6))\n",
        "#ax = fig.add_subplot(111, projection='3d')\n",
        "#ax.plot_surface(Teval, Xeval, Exact, cmap='jet');\n",
        "#ax.view_init(35,35)\n",
        "#ax.set_xlabel('$t$')\n",
        "#ax.set_ylabel('$x$')\n",
        "#ax.set_zlabel('$u(t,x)$')\n",
        "#ax.set_title('Exact solution of wave equation \\n');\n",
        "#plt.savefig('Exact solution of wave equation.jpeg', bbox_inches='tight', dpi=300);\n",
        "\n",
        "# Initialize model aka u_\\theta\n",
        "for a in range(len(num_train_domain)):\n",
        "    N_r = num_train_domain[a]\n",
        "    # Draw uniformly sampled collocation points\n",
        "    t_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
        "    x_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
        "    X_r = tf.concat([t_r, x_r], axis=1)\n",
        "    # Collect boundary and inital data in lists\n",
        "    #X_data = [X_0, X_b]\n",
        "    #u_data = [u_0, u_b]\n",
        "    #fig = plt.figure(figsize=(9,6))\n",
        "    #plt.scatter(t_r, x_r, c='black', marker='.',alpha=0.3)\n",
        "    #plt.xlabel('$t$')\n",
        "    #plt.ylabel('$x$')\n",
        "    #plt.title('Positions of train points');\n",
        "    #plt.savefig('Positions of train points.jpeg', bbox_inches='tight', dpi=300)\n",
        "    for b in range(len(activation)):\n",
        "      activation_func = activation[b]\n",
        "      for c in range(len(optimizer)):\n",
        "        for d in range(len(learn_rate)):\n",
        "          learning_rate_func = learn_rate[d]\n",
        "          for e in range(len(N_epoch)):\n",
        "            N_epoch_func = N_epoch[e]\n",
        "            for f in range(len(num_neurons_per_layer)):\n",
        "              num_neurons_per_layer_func = num_neurons_per_layer[f]\n",
        "              for g in range(len(num_hidden_layers)):\n",
        "                    Result[i,0] = i + 0\n",
        "                    Result[i,1] = a\n",
        "                    Result[i,2] = b\n",
        "                    Result[i,3] = c\n",
        "                    Result[i,4] = d \n",
        "                    Result[i,5] = e \n",
        "                    Result[i,6] = f \n",
        "                    Result[i,7] = g \n",
        "                    num_hidden_layers_func = num_hidden_layers[g]\n",
        "                    # Define one training step as a TensorFlow function to increase speed of training\n",
        "                    @tf.function\n",
        "                    def train_step(model):\n",
        "                      loss, grad_theta = get_grad(model, X_r)\n",
        "                      # Perform gradient descent step\n",
        "                      optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
        "                      return loss\n",
        "                    model = init_model(num_hidden_layers_func,num_neurons_per_layer_func)\n",
        "                    # Choose the optimizer\n",
        "                    if optimizer[c] == 'Adam':\n",
        "                      optim = tf.keras.optimizers.Adam(learning_rate_func)\n",
        "                    if optimizer[c] == 'SGD':\n",
        "                      optim = tf.keras.optimizers.SGD(learning_rate_func)\n",
        "                    if optimizer[c] == 'RMSprop':\n",
        "                      optim = tf.keras.optimizers.RMSprop(learning_rate_func)\n",
        "                    if optimizer[c] == 'Adadelta':\n",
        "                      optim = tf.keras.optimizers.Adadelta(learning_rate_func)\n",
        "                    #hist = []\n",
        "                    # Start timer\n",
        "                    t0 = time()\n",
        "                    for k in range(N_epoch_func+1):\n",
        "                        loss = train_step(model)\n",
        "                        # Append current loss to hist\n",
        "                        #hist.append(loss.numpy())\n",
        "                        # Output current loss after 50 iterates\n",
        "                        #if k%50 == 0:\n",
        "                          #print('It {:05d}: loss = {:10.8e}'.format(k,loss))\n",
        "                    Loss_final = loss.numpy()\n",
        "                    # Print cases number\n",
        "                    print('Cases number: ',i)\n",
        "                    # Print computation time and Loss\n",
        "                    Time_final = time()-t0\n",
        "                    print('Computation time:(seconds) ',Time_final)\n",
        "                    print('Final loss: ',Loss_final)\n",
        "\n",
        "                    # Evaluation predicted solutions\n",
        "                    ueval = (1 - teval**2)*u_0(xeval) + teval*v_t_0(xeval) + xeval*(1-xeval)*teval**2*model(tf.cast(Eval_grid,DTYPE))\n",
        "                    # Reshape ueval\n",
        "                    Ueval = ueval.numpy().reshape(num_eval,num_eval)\n",
        "                    # Error evaluation\n",
        "                    Error = np.abs(Ueval - Exact)\n",
        "                    L2_error = np.linalg.norm(Error,2)/np.linalg.norm(Exact,2)\n",
        "                    print('Relative L2 error: ',L2_error)\n",
        "\n",
        "                    # Assign reuslts\n",
        "                    Result[i,8]= Time_final\n",
        "                    Result[i,9]= Loss_final\n",
        "                    Result[i,10]= L2_error\n",
        "                    i +=1\n",
        "                    # Plot of loss function\n",
        "                    #fig = plt.figure(figsize=(9,6))\n",
        "                    #ax = fig.add_subplot(111)\n",
        "                    #ax.semilogy(range(len(hist)), hist,'k-')\n",
        "                    #ax.set_xlabel('$n_{epoch}$')\n",
        "                    #ax.set_ylabel('$Loss$');\n",
        "                    #ax.set_title('Loss of the trained neural networks \\n');\n",
        "                    #plt.savefig('Loss of the trained neural networks.jpeg', bbox_inches='tight', dpi=300);\n",
        "\n",
        "                    # Surface plot of predicted solution u(t,x)\n",
        "                    #fig = plt.figure(figsize=(9,6))\n",
        "                    #ax = fig.add_subplot(111, projection='3d')\n",
        "                    #ax.plot_surface(Teval, Xeval, Ueval, cmap='jet');\n",
        "                    #ax.view_init(35,35)\n",
        "                    #ax.set_xlabel('$t$')\n",
        "                    #ax.set_ylabel('$x$')\n",
        "                    #ax.set_zlabel('$u(t,x)$')\n",
        "                    #ax.set_title('Predicted solution of wave equation \\n');\n",
        "                    #plt.savefig('Predicted solution of wave equation.jpeg', bbox_inches='tight', dpi=300);\n",
        "\n",
        "                    # Surface plot of absolute error\n",
        "                    #fig = plt.figure(figsize=(9,6))\n",
        "                    #ax = fig.add_subplot(111, projection='3d')\n",
        "                    #ax.plot_surface(Teval, Xeval, Error, cmap='jet');\n",
        "                    #ax.view_init(35,35)\n",
        "                    #ax.set_xlabel('$t$')\n",
        "                    #ax.set_ylabel('$x$')\n",
        "                    #ax.set_zlabel('$u_\\\\theta(t,x)$')\n",
        "                    #ax.set_title('Difference between predicted and exact solutions of wave equation \\n');\n",
        "                    #plt.savefig('Difference between predicted and exact solutions of wave equation.jpeg', bbox_inches='tight', dpi=300);\n",
        "\n",
        "np.savetxt('Wave-Summary.csv', Result, delimiter=',')\n",
        "#Download result file\n",
        "files.download('Wave-Summary.csv') \n"
      ]
    }
  ]
}