{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Burger equation - Grid Search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBoaYn1jU2P5jhUOCK8OuG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkle91/Hyperparameter_Optimization_for_PINNs_using_GA/blob/main/Burger_equation_Grid_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaXh3j3uLPI4"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow and NumPy\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#import sys\n",
        "#sys.path.insert(0, 'E:/Thesis/PINNs/Genetic_Agorithm/')\n",
        "import scipy.io\n",
        "from keras import models \n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# Set data type\n",
        "DTYPE='float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "\n",
        "# Set constants\n",
        "pi = tf.constant(np.pi, dtype=DTYPE)\n",
        "viscosity = .01/pi\n",
        "\n",
        "# Define initial condition\n",
        "def fun_u_0(x):\n",
        "    return -tf.sin(pi * x)\n",
        "\n",
        "# Define boundary condition\n",
        "def fun_u_b(t, x):\n",
        "    n = x.shape[0]\n",
        "    return tf.zeros((n,1), dtype=DTYPE)\n",
        "\n",
        "# Define residual of the PDE\n",
        "def fun_r(t, x, u, u_t, u_x, u_xx):\n",
        "    return u_t + u * u_x - viscosity * u_xx\n",
        "\n",
        "# Set boundary\n",
        "tmin = 0.\n",
        "tmax = 1.\n",
        "xmin = -1.\n",
        "xmax = 1.\n",
        "\n",
        "# Lower bounds\n",
        "lb = tf.constant([tmin, xmin], dtype=DTYPE)\n",
        "# Upper bounds\n",
        "ub = tf.constant([tmax, xmax], dtype=DTYPE)\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Result management\n",
        "Result = np.ones((6561, 12))*999999\n",
        "i= 0\n",
        "\n",
        "# 1. Number of train points in IBCs: 0-50, 1-100, 2-150\n",
        "num_train_BC = [50, 100, 150]\n",
        "# 2. Number of train points in domain: 0-5000, 1-10000, 2-15000\n",
        "num_train_domain = [5000, 10000, 15000]\n",
        "# 3. Activation function: 0-tanh, 1-sigmoid, 2-Relu\n",
        "activation = ['tanh', 'sigmoid', 'relu']\n",
        "# 4. Optimizer: 0-Adam, 1-SGD, 2-RMSprop\n",
        "optimizer = ['Adam', 'SGD', 'RMSprop']\n",
        "# 5. Learning rate: 0-1e-2, 1-1e-3, 2-1e-4\n",
        "learn_rate = [1e-02, 1e-03, 1e-04]\n",
        "# 6. Number of epochs: 0-2500, 1-5000, 2-7500\n",
        "N_epoch = [2500, 5000, 7500]\n",
        "# 7. Number of neurons: 0-10, 1-20, 2-30\n",
        "num_neurons_per_layer = [10, 20, 30]\n",
        "# 8. Number of layers: 0-6, 1-8, 2-10\n",
        "num_hidden_layers = [6, 8, 10]\n",
        "\n",
        "\n",
        "def init_model(num_hidden_layers, num_neurons_per_layer):\n",
        "    # Initialize a feedforward neural network\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Input is two-dimensional (time + one spatial dimension)\n",
        "    model.add(tf.keras.Input(2))\n",
        "\n",
        "    # Introduce a scaling layer to map input to [lb, ub]\n",
        "    scaling_layer = tf.keras.layers.Lambda(\n",
        "                lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
        "    model.add(scaling_layer)\n",
        "\n",
        "    # Append hidden layers\n",
        "    for _ in range(num_hidden_layers):\n",
        "        model.add(tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "            activation=tf.keras.activations.get(activation_func),\n",
        "            kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Output is one-dimensional\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    \n",
        "    return model\n",
        "\n",
        "def get_r(model, X_r):\n",
        "    # A tf.GradientTape is used to compute derivatives in TensorFlow\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Split t and x to compute partial derivatives\n",
        "        t, x = X_r[:, 0:1], X_r[:,1:2]\n",
        "\n",
        "        # Variables t and x are watched during tape\n",
        "        # to compute derivatives u_t and u_x\n",
        "        tape.watch(t)\n",
        "        tape.watch(x)\n",
        "\n",
        "        # Determine residual \n",
        "        u = model(tf.stack([t[:,0], x[:,0]], axis=1))\n",
        "\n",
        "        # Compute gradient u_x within the GradientTape\n",
        "        # since we need second derivatives\n",
        "        u_x = tape.gradient(u, x)        \n",
        "    u_t = tape.gradient(u, t)\n",
        "    u_xx = tape.gradient(u_x, x)\n",
        "    del tape\n",
        "    return fun_r(t, x, u, u_t, u_x, u_xx)\n",
        "\n",
        "def compute_loss(model, X_r, X_data, u_data):\n",
        "    # Compute phi^r\n",
        "    r = get_r(model, X_r)\n",
        "    phi_r = tf.reduce_mean(tf.square(r))\n",
        "    # Initialize loss\n",
        "    loss = phi_r\n",
        "    # Add phi^0 and phi^b to the loss\n",
        "    for i in range(len(X_data)):\n",
        "        u_pred = model(X_data[i])\n",
        "        loss += tf.reduce_mean(tf.square(u_data[i] - u_pred))\n",
        "    return loss\n",
        "\n",
        "def get_grad(model, X_r, X_data, u_data):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # This tape is for derivatives with\n",
        "        # respect to trainable variables\n",
        "        tape.watch(model.trainable_variables)\n",
        "        loss = compute_loss(model, X_r, X_data, u_data)\n",
        "    g = tape.gradient(loss, model.trainable_variables)\n",
        "    del tape\n",
        "    return loss, g\n",
        "\n",
        "# Preparation for predicted solutions of test points\n",
        "data = scipy.io.loadmat('burgers_shock.mat')\n",
        "teval = data['t'].flatten()[:,None]\n",
        "xeval = data['x'].flatten()[:,None]\n",
        "Exact = np.real(data['usol']) \n",
        "Teval, Xeval = np.meshgrid(teval, xeval)\n",
        "Xeval_grid = np.vstack([Teval.flatten(),Xeval.flatten()]).T\n",
        "\n",
        "# Surface plot of analytical solution u(t,x)\n",
        "#fig = plt.figure(figsize=(9,6))\n",
        "#ax = fig.add_subplot(111, projection='3d')\n",
        "#ax.plot_surface(Teval, Xeval, Exact, cmap='jet');\n",
        "#ax.view_init(35,35)\n",
        "#ax.set_xlabel('$t$')\n",
        "#ax.set_ylabel('$x$')\n",
        "#ax.set_zlabel('$u(t,x)$')\n",
        "#ax.set_title('Exact solution of Burgers equation \\n');\n",
        "#plt.savefig('Exact solution of Burgers equation.jpeg', bbox_inches='tight', dpi=300);\n",
        "\n",
        "# Initialize model aka u_\\theta\n",
        "for a in range(len(num_train_BC)):\n",
        "  N_0 = N_b = num_train_BC[a]\n",
        "  # Draw uniform sample points for initial boundary data\n",
        "  t_0 = tf.ones((N_0,1), dtype=DTYPE)*lb[0]\n",
        "  x_0 = tf.random.uniform((N_0,1), lb[1], ub[1], dtype=DTYPE)\n",
        "  X_0 = tf.concat([t_0, x_0], axis=1)\n",
        "  # Evaluate intitial condition at x_0\n",
        "  u_0 = fun_u_0(x_0)\n",
        "  # Boundary data\n",
        "  t_b = tf.random.uniform((N_b,1), lb[0], ub[0], dtype=DTYPE)\n",
        "  x_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
        "  X_b = tf.concat([t_b, x_b], axis=1)\n",
        "  # Evaluate boundary condition at (t_b,x_b)\n",
        "  u_b = fun_u_b(t_b, x_b)\n",
        "  for b in range(len(num_train_domain)):\n",
        "    N_r = num_train_domain[b]\n",
        "    # Draw uniformly sampled collocation points\n",
        "    t_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
        "    x_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
        "    X_r = tf.concat([t_r, x_r], axis=1)\n",
        "    # Collect boundary and inital data in lists\n",
        "    X_data = [X_0, X_b]\n",
        "    u_data = [u_0, u_b]\n",
        "    #fig = plt.figure(figsize=(9,6))\n",
        "    #plt.scatter(t_0, x_0, c='b', marker='X',alpha=1 )\n",
        "    #plt.scatter(t_b, x_b, c='r', marker='X',alpha=1)\n",
        "    #plt.scatter(t_r, x_r, c='black', marker='.',alpha=0.3)\n",
        "    #plt.xlabel('$t$')\n",
        "    #plt.ylabel('$x$')\n",
        "    #plt.title('Positions of train points');\n",
        "    #plt.savefig('Positions of train points.jpeg', bbox_inches='tight', dpi=300)\n",
        "    for c in range(len(activation)):\n",
        "      activation_func = activation[c]\n",
        "      for d in range(len(optimizer)):\n",
        "        for e in range(len(learn_rate)):\n",
        "          learning_rate_func = learn_rate[e]\n",
        "          for f in range(len(N_epoch)):\n",
        "            N_epoch_func = N_epoch[f]\n",
        "            for g in range(len(num_neurons_per_layer)):\n",
        "              num_neurons_per_layer_func = num_neurons_per_layer[g]\n",
        "              for h in range(len(num_hidden_layers)):\n",
        "                    Result[i,0] = i\n",
        "                    Result[i,1] = a\n",
        "                    Result[i,2] = b\n",
        "                    Result[i,3] = c\n",
        "                    Result[i,4] = d \n",
        "                    Result[i,5] = e \n",
        "                    Result[i,6] = f \n",
        "                    Result[i,7] = g \n",
        "                    Result[i,8] = h \n",
        "                    num_hidden_layers_func = num_hidden_layers[h]\n",
        "                    # Define one training step as a TensorFlow function to increase speed of training\n",
        "                    @tf.function\n",
        "                    def train_step(model):\n",
        "                      loss, grad_theta = get_grad(model, X_r, X_data, u_data)\n",
        "                      # Perform gradient descent step\n",
        "                      optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
        "                      return loss\n",
        "                    model = init_model(num_hidden_layers_func,num_neurons_per_layer_func)\n",
        "                    # Choose the optimizer\n",
        "                    if optimizer[d] == 'Adam':\n",
        "                      optim = tf.keras.optimizers.Adam(learning_rate_func)\n",
        "                    if optimizer[d] == 'SGD':\n",
        "                      optim = tf.keras.optimizers.SGD(learning_rate_func)\n",
        "                    if optimizer[d] == 'RMSprop':\n",
        "                      optim = tf.keras.optimizers.RMSprop(learning_rate_func)\n",
        "                    hist = []\n",
        "                    # Start timer\n",
        "                    t0 = time()\n",
        "                    for k in range(N_epoch_func+1):\n",
        "                        loss = train_step(model)\n",
        "                        # Append current loss to hist\n",
        "                        #hist.append(loss.numpy())\n",
        "                        # Output current loss after 50 iterates\n",
        "                        #if i%50 == 0:\n",
        "                          #print('It {:05d}: loss = {:10.8e}'.format(i,loss))\n",
        "                    Loss_final = loss.numpy()\n",
        "                    # Print cases number\n",
        "                    print('Cases number: ',i)\n",
        "                    # Print computation time and Loss\n",
        "                    Time_final = time()-t0\n",
        "                    print('Computation time:(seconds) ',Time_final)\n",
        "                    print('Final loss: ',Loss_final)\n",
        "\n",
        "                    # Evaluation predicted solutions\n",
        "                    ueval = model(tf.cast(Xeval_grid,DTYPE))\n",
        "                    # Reshape ueval\n",
        "                    Ueval = ueval.numpy().reshape(xeval.shape[0],teval.shape[0])\n",
        "                    # Error evaluation\n",
        "                    Error = np.abs(Ueval - Exact)\n",
        "                    L2_error = np.linalg.norm(Error,2)/np.linalg.norm(Exact,2)\n",
        "                    print('Relative L2 error: ',L2_error)\n",
        "\n",
        "                    Result[i,0]= i\n",
        "                    Result[i,9]= Time_final\n",
        "                    Result[i,10]= Loss_final\n",
        "                    Result[i,11]= L2_error\n",
        "                    i +=1\n",
        "                    # Plot of loss function\n",
        "                    #fig = plt.figure(figsize=(9,6))\n",
        "                    #ax = fig.add_subplot(111)\n",
        "                    #ax.semilogy(range(len(hist)), hist,'k-')\n",
        "                    #ax.set_xlabel('$n_{epoch}$')\n",
        "                    #ax.set_ylabel('$Loss$');\n",
        "                    #ax.set_title('Loss of the trained neural networks \\n');\n",
        "                    #plt.savefig('Loss of the trained neural networks.jpeg', bbox_inches='tight', dpi=300);\n",
        "\n",
        "                    # Surface plot of predicted solution u(t,x)\n",
        "                    #fig = plt.figure(figsize=(9,6))\n",
        "                    #ax = fig.add_subplot(111, projection='3d')\n",
        "                    #ax.plot_surface(Teval, Xeval, Ueval, cmap='jet');\n",
        "                    #ax.view_init(35,35)\n",
        "                    #ax.set_xlabel('$t$')\n",
        "                    #ax.set_ylabel('$x$')\n",
        "                    #ax.set_zlabel('$u(t,x)$')\n",
        "                    #ax.set_title('Predicted solution of Burgers equation \\n');\n",
        "                    #plt.savefig('Predicted solution of Burgers equation.jpeg', bbox_inches='tight', dpi=300);\n",
        "\n",
        "                    # Surface plot of absolute error\n",
        "                    #fig = plt.figure(figsize=(9,6))\n",
        "                    #ax = fig.add_subplot(111, projection='3d')\n",
        "                    #ax.plot_surface(Teval, Xeval, Error, cmap='jet');\n",
        "                    #ax.view_init(35,35)\n",
        "                    #ax.set_xlabel('$t$')\n",
        "                    #ax.set_ylabel('$x$')\n",
        "                    #ax.set_zlabel('$u_\\\\theta(t,x)$')\n",
        "                    #ax.set_title('Difference between predicted and exact solutions of Burgers equation \\n');\n",
        "                    #plt.savefig('Difference between predicted and exact solutions of Burgers equation.jpeg', bbox_inches='tight', dpi=300);\n",
        "np.savetxt('Burger-Summary.csv', Result, delimiter=',')\n",
        "#Download result file\n",
        "files.download('Burger-Summary.csv')\n"
      ]
    }
  ]
}